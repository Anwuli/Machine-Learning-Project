from sklearn.model_selection import train_test_split
import numpy as np
from imbal.util.constants import ModelType
from imbal.util.simple_dataset import SimpleDataset

def split(
        x_set,
        y_set,
        sample_weights=None,
        test_size=None,
        train_size=None,
        seed=None,
        shuffle=True,
        mode=ModelType.CLASSIFICATION,
    ) -> tuple:
    """

    Args:
        x_set: A NumPy array of data points, arranged as a column vector
        y_set: A NumPy array of labels, arranged as a column vector
        sample_weights: Optional, default :code:`None`. A NumPy array of weights,
            arranged as a column vector.
        test_size: Optional, default :code:`None`. The percentage of data that
            should be contained in the test split.
        train_size: Optional, default :code:`None`. The percentage of data that
            should be contained in the train split. If specified, overrides the
            value of :code:`test_size`.
        seed: Optional, default :code:`None`. The random seed to use for shuffling.
            When set to :code:`None`, a seed is generated by the system.
        shuffle: Optional, default :code:`True`. Whether to shuffle the data before splitting.
        mode: Optional, default :code:`'classification'` Should be set to :code:`'classification'`
            when working with discrete labels (classes), and :code:`'regression'` when working
            with continuous labels, as the splitting process differs depending on the label type.

    Returns:
        Two tuples, in the form :code:`(train_x, train_y), (test_x, test_y)` when weights are
        not specified, and :code:`(train_x, train_y, train_weights), (test_x, test_y, test_weights)`
        when weights are specified

    In :code:`classification` mode, data is stratified by class, ensuring that data is spread
    as closely to the specified split percentage as possible across the train and test splits.
    In :code:`regression` mode, there are no explict classes to stratify data on. Instead,
    the data is sorted based on its label, then seperated into pseudo classes, which are then split based on
    the specified split percentage. These psuedo-classes are of size 10 or 100 based on the
    specified split percentage, therefore it is encouraged that the split percentage is a
    multiple of 10% for smaller data sets, and a multiple of 1% for larger data sets.

    Example:

     .. code-block:: python

        >>> data = np.arange(21).reshape(-1,1)
        >>> labels = np.arange(21).reshape(-1,1)
        >>> weights = (np.ones(21) / 21).reshape(-1, 1)

        >>> train_set, test_set = imbal.stratified_sampling.split(data, labels, weights, test_size=0.20, mode='regression')
        >>> x_train, y_train, w_train = train_set
        >>> x_test, y_test, w_test = test_set

        >>> print(x_train)
        [20. 16.  8.  2. 14.  1. 11.  9. 19.  0.  5. 13. 15.  6. 17. 12.  7.]
        >>> print(y_train)
        [10.  3.  4. 18.]

    """

    if mode == ModelType.REGRESSION:
        x_train, y_train, w_train, x_test, y_test, w_test = _stratified_regression_split(
            x_set,
            y_set,
            sample_weights=sample_weights,
            test_size=test_size,
            train_size=train_size
        )

        if shuffle:
            rng = np.random.default_rng(seed)
            train_indices = np.arange(len(x_train))
            rng.shuffle(train_indices)
            test_indices = np.arange(len(x_test))
            rng.shuffle(test_indices)

            x_train = np.array(x_train)[train_indices]
            y_train = np.array(y_train)[train_indices]
            if w_train is not None:
                w_train = np.array(w_train)[train_indices]
            x_test = np.array(x_test)[test_indices]
            y_test = np.array(y_test)[test_indices]
            if w_test is not None:
                w_test = np.array(w_test)[test_indices]
        if w_train is not None:
            return SimpleDataset(x_train, y_train, w_train), SimpleDataset(x_test, y_test, w_test)
        else:
            return SimpleDataset(x_train, y_train), SimpleDataset(x_test, y_test)
    else:
        if shuffle:
            rng = np.random.default_rng(seed)
            indices = np.arange(x_set.shape[0])
            rng.shuffle(indices)
            x_set = x_set[indices]
            y_set = y_set[indices]
            sample_weights = sample_weights[indices]

        if sample_weights is None:
            x_train, x_test, y_train, y_test = train_test_split(
                x_set,
                y_set,
                test_size=test_size,
                train_size=train_size,
                random_state=seed,
                stratify=y_set
            )
            return SimpleDataset(x_train, y_train), SimpleDataset(x_test, y_test)
        else:
            x_train, x_test, y_train, y_test, w_train, w_test = train_test_split(
                x_set,
                y_set,
                sample_weights,
                test_size=test_size,
                train_size=train_size,
                random_state=seed,
                stratify=y_set
            )
            return SimpleDataset(x_train, y_train, w_train), SimpleDataset(x_test, y_test, w_test)

def _stratified_regression_split(
        x_set,
        y_set,
        sample_weights,
        test_size=None,
        train_size=None
) -> list:

    if train_size is None:
        train_size = 1 - test_size

    array_length = x_set.shape[0]
    if array_length != y_set.shape[0]:
        raise ValueError('Length of all passed arrays must be equal')

    sort_order = np.argsort(y_set.reshape(-1,))
    x_set = x_set[sort_order]
    y_set = y_set[sort_order]
    sample_weights = sample_weights[sort_order]

    train_size = round(train_size, 2)
    train_per_batch = round(100*train_size)
    if abs(train_size * 10 - round(train_size * 10)) < 1e-6:
        batch_size = 10
        train_per_batch = round(10*train_size)
    else:
        batch_size = 100


    train_split_arrays = []
    test_split_arrays = []

    combined_arrays = np.array([x_set, y_set, sample_weights])

    for i in range(array_length // batch_size):
        batch = combined_arrays[:, i*batch_size:(i+1)*batch_size]
        indices = np.arange(batch_size)
        np.random.shuffle(indices)
        batch = batch[:, indices]
        train_split_arrays.append(batch[:, :train_per_batch])
        test_split_arrays.append(batch[:, train_per_batch:])

    if array_length / batch_size != round(array_length / batch_size):
        batch = combined_arrays[:, array_length // batch_size * batch_size:]
        partial_batch_size = len(batch[0])
        partial_train = round(partial_batch_size * train_size)
        indices = np.arange(partial_batch_size)
        np.random.shuffle(indices)
        batch = batch[:, indices]
        train_split_arrays.append(batch[:, :partial_train])
        if partial_train != partial_batch_size:
            test_split_arrays.append(batch[:, partial_train:])

    train_split_arrays = np.concatenate(train_split_arrays, axis=1)
    indices = np.arange(train_split_arrays.shape[1])
    np.random.shuffle(indices)
    train_split_arrays = train_split_arrays[:, indices]

    test_split_arrays = np.concatenate(test_split_arrays, axis=1)
    indices = np.arange(test_split_arrays.shape[1])
    np.random.shuffle(indices)
    test_split_arrays = test_split_arrays[:, indices]

    return train_split_arrays.tolist() + test_split_arrays.tolist()
        
