# -*- coding: utf-8 -*-
"""ModelWithClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nww7Ec2zPKreUK90swb5xbkvcjn8aRTy
"""
import os
# Models already downloaded locally - no need to run gdown
# os.system("gdown --folder 'https://drive.google.com/drive/folders/128YocnfqwhL5qwZ_02D8UotbfBDMV5u-?usp=drive_link'")

# Commented out IPython magic to ensure Python compatibility.
# %pip install medmnist
# %pip install torchinfo

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision.models import resnet18, ResNet18_Weights
from torchvision import transforms
from torch.utils.data import DataLoader
from medmnist import OCTMNIST
import numpy as np
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
import copy

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============================================================================
# CALIBRATION COMPONENTS (From Your Original Code)
# ============================================================================

class ECELoss(nn.Module):
    """Expected Calibration Error - measures calibration quality."""
    def __init__(self, n_bins=15):
        super().__init__()
        bin_boundaries = torch.linspace(0, 1, n_bins + 1)
        self.bin_lowers = bin_boundaries[:-1]
        self.bin_uppers = bin_boundaries[1:]

    def forward(self, logits, labels):
        softmaxes = F.softmax(logits, dim=1)
        confidences, predictions = torch.max(softmaxes, 1)
        accuracies = predictions.eq(labels)

        ece = torch.zeros(1, device=logits.device)
        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):
            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())
            prop_in_bin = in_bin.float().mean()
            if prop_in_bin.item() > 0:
                accuracy_in_bin = accuracies[in_bin].float().mean()
                avg_confidence_in_bin = confidences[in_bin].mean()
                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
        return ece


class TemperatureScaling(nn.Module):
    """
    Temperature scaling calibration.
    After calibration, confidence scores better reflect true accuracy.
    """
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.temperature = nn.Parameter(torch.ones(1) * 1.5)

    def forward(self, x):
        logits = self.model(x)
        return logits / self.temperature

    def calibrate(self, val_loader, device):
        """Tune temperature on validation set."""
        self.to(device)
        self.model.eval()

        # Collect all logits and labels
        logits_list, labels_list = [], []
        with torch.no_grad():
            for inputs, labels in tqdm(val_loader, desc="Collecting logits"):
                logits = self.model(inputs.to(device))
                logits_list.append(logits)
                labels_list.append(labels)

        logits = torch.cat(logits_list).to(device)
        labels = torch.cat(labels_list).view(-1).long().to(device)

        # Optimize temperature
        nll = nn.CrossEntropyLoss()
        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=100)

        def eval_loss():
            optimizer.zero_grad()
            loss = nll(logits / self.temperature, labels)
            loss.backward()
            return loss

        optimizer.step(eval_loss)

        # Report results
        ece = ECELoss()
        print(f"Optimal temperature: {self.temperature.item():.4f}")
        print(f"ECE before: {ece(logits, labels).item():.4f}")
        print(f"ECE after: {ece(logits / self.temperature, labels).item():.4f}")

        return self

# ============================================================================
# APPROACH 1: CALIBRATED CONFIDENCE REJECTION (Simple Baseline)
# ============================================================================

class CalibratedConfidenceRejector:
    """
    Simple approach: Use calibrated confidence scores for rejection.

    1. Apply temperature scaling to get calibrated probabilities
    2. Reject samples where max probability < threshold

    Pros: Simple, no additional training needed
    Cons: May not be optimal for selective prediction
    """

    def __init__(self, calibrated_model):
        self.model = calibrated_model

    def predict(self, x, threshold=0.5):
        """
        Returns:
            predictions: Class predictions
            confidences: Calibrated confidence scores
            accepted: Boolean mask
        """
        self.model.eval()
        with torch.no_grad():
            logits = self.model(x)
            probs = F.softmax(logits, dim=1)
            confidences, predictions = torch.max(probs, dim=1)
            accepted = confidences >= threshold

        return predictions, confidences, accepted

# ============================================================================
# APPROACH 2: CALIBRATED SELECTIVENET (Advanced)
# ============================================================================

class CalibratedSelectiveNet(nn.Module):
    """
    Combines:
    1. Temperature-scaled prediction head (calibrated outputs)
    2. Learned selection head (optimal accept/reject)
    3. Auxiliary head (training regularization)

    This is the FULL implementation of your hypothesis.
    """

    def __init__(self, base_model, num_classes=4, feature_dim=512, temperature=1.0):
        super().__init__()

        # Feature extractor
        self.features = nn.Sequential(*list(base_model.children())[:-1])

        # Prediction head with temperature scaling
        # Handle both Sequential and Linear fc layers
        if isinstance(base_model.fc, nn.Sequential):
            # Deep copy the entire Sequential as prediction head
            self.prediction_head = copy.deepcopy(base_model.fc)
        else:
            self.prediction_head = nn.Linear(feature_dim, num_classes)
            if hasattr(base_model, 'fc'):
                self.prediction_head.weight.data = base_model.fc.weight.data.clone()
                self.prediction_head.bias.data = base_model.fc.bias.data.clone()

        # Learnable temperature for calibration
        self.temperature = nn.Parameter(torch.tensor(temperature))

        # Selection head
        self.selection_head = nn.Sequential(
            nn.Linear(feature_dim, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Linear(512, 1),
            nn.Sigmoid()
        )

        # Auxiliary head
        self.auxiliary_head = nn.Linear(feature_dim, num_classes)

    def forward(self, x):
        features = self.features(x).view(x.size(0), -1)

        # Calibrated prediction
        logits = self.prediction_head(features)
        calibrated_logits = logits / self.temperature

        # Selection score
        selection = self.selection_head(features)

        # Auxiliary
        auxiliary = self.auxiliary_head(features)

        return calibrated_logits, selection, auxiliary, logits

    def predict_selective(self, x, threshold=0.5):
        self.eval()
        with torch.no_grad():
            cal_logits, selection, _, _ = self.forward(x)
            probs = F.softmax(cal_logits, dim=1)
            predictions = torch.argmax(probs, dim=1)
            confidences, _ = torch.max(probs, dim=1)
            selection = selection.squeeze()
            accepted = selection >= threshold

        return predictions, confidences, selection, accepted


class CalibratedSelectiveLoss(nn.Module):
    """
    Combined loss for calibration + selection.

    Total = Î± * L_selective + (1-Î±) * L_auxiliary + Î² * L_calibration

    Where L_calibration encourages temperature to improve ECE.
    """

    def __init__(self, coverage=0.8, lmbda=32, alpha=0.5, beta=0.1):
        super().__init__()
        self.coverage = coverage
        self.lmbda = lmbda
        self.alpha = alpha
        self.beta = beta
        self.ece = ECELoss()

    def forward(self, cal_logits, selection, auxiliary, raw_logits, targets):
        targets = targets.view(-1).long()
        g = selection.view(-1)

        # Selective loss
        ce = F.cross_entropy(cal_logits, targets, reduction='none')
        selective_risk = (g * ce).sum() / (g.sum() + 1e-8)
        coverage_penalty = self.lmbda * F.relu(self.coverage - g.mean()) ** 2
        selective_loss = selective_risk + coverage_penalty

        # Auxiliary loss
        auxiliary_loss = F.cross_entropy(auxiliary, targets)

        # Calibration loss (ECE on raw logits to encourage good temperature)
        calibration_loss = self.ece(cal_logits, targets)

        total = (self.alpha * selective_loss +
                 (1 - self.alpha) * auxiliary_loss +
                 self.beta * calibration_loss)

        return total, {
            'selective': selective_loss.item(),
            'auxiliary': auxiliary_loss.item(),
            'calibration': calibration_loss.item(),
            'coverage': g.mean().item()
        }

# ============================================================================
# TRAINING FUNCTION - IMPROVED HYPERPARAMETERS
# ============================================================================

def train_calibrated_selectivenet(model, train_loader, val_loader, device,
                                   epochs=75, lr=5e-4, coverage=0.8, weight_decay=1e-4):
    """Train the full calibrated selective model with improved hyperparameters."""

    model = model.to(device)
    
    # Improved optimizer with weight decay for regularization
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    
    # Cosine annealing with warm restarts for better convergence
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2, eta_min=1e-6)
    
    criterion = CalibratedSelectiveLoss(coverage=coverage, lmbda=32, alpha=0.5, beta=0.1)

    history = {'train_loss': [], 'val_acc': [], 'coverage': [], 'ece': []}
    
    best_val_acc = 0.0
    best_model_state = None
    patience_counter = 0
    patience = 15  # Early stopping patience

    for epoch in range(epochs):
        model.train()
        epoch_loss, epoch_cov = 0, 0

        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            cal_logits, selection, auxiliary, raw_logits = model(images)
            loss, metrics = criterion(cal_logits, selection, auxiliary, raw_logits, labels)
            loss.backward()
            
            # Gradient clipping for stability
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()

            epoch_loss += loss.item()
            epoch_cov += metrics['coverage']
            pbar.set_postfix({'loss': f"{loss.item():.3f}", 'cov': f"{metrics['coverage']:.3f}", 'lr': f"{optimizer.param_groups[0]['lr']:.2e}"})

        scheduler.step()

        # Validation
        model.eval()
        val_correct, val_total = 0, 0
        val_logits, val_labels = [], []

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                cal_logits, _, _, _ = model(images)
                _, preds = torch.max(cal_logits, 1)
                val_total += labels.size(0)
                val_correct += (preds == labels.view(-1)).sum().item()
                val_logits.append(cal_logits)
                val_labels.append(labels.view(-1))

        val_acc = val_correct / val_total
        val_logits = torch.cat(val_logits)
        val_labels = torch.cat(val_labels)
        ece = ECELoss()(val_logits, val_labels).item()

        history['train_loss'].append(epoch_loss / len(train_loader))
        history['val_acc'].append(val_acc)
        history['coverage'].append(epoch_cov / len(train_loader))
        history['ece'].append(ece)

        print(f"  Val Acc: {val_acc:.4f} | ECE: {ece:.4f} | Temp: {model.temperature.item():.3f} | LR: {optimizer.param_groups[0]['lr']:.2e}")
        
        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            patience_counter = 0
            print(f"  âœ“ New best model saved (val_acc: {best_val_acc:.4f})")
        else:
            patience_counter += 1
            
        # Early stopping check
        if patience_counter >= patience:
            print(f"\nEarly stopping triggered after {epoch+1} epochs")
            break
    
    # Load best model
    if best_model_state is not None:
        model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})
        print(f"\nLoaded best model with val_acc: {best_val_acc:.4f}")

    return model, history

# ============================================================================
# COMPREHENSIVE EVALUATION - TESTS YOUR HYPOTHESIS
# ============================================================================

def evaluate_hypothesis(model, test_loader, device, method='selectivenet'):
    """
    Comprehensive evaluation to test the hypothesis.

    Measures:
    1. Baseline accuracy (no rejection)
    2. Selective accuracy at various coverage levels
    3. ECE (calibration quality)
    4. Risk-coverage trade-off
    """

    model.eval()
    all_preds, all_labels, all_confs, all_selections = [], [], [], []
    all_logits = []

    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc='Evaluating'):
            images = images.to(device)

            if method == 'selectivenet':
                cal_logits, selection, _, _ = model(images)
                probs = F.softmax(cal_logits, dim=1)
                confs, preds = torch.max(probs, dim=1)
                selections = selection.squeeze()
            else:  # confidence-based
                logits = model(images)
                probs = F.softmax(logits, dim=1)
                confs, preds = torch.max(probs, dim=1)
                selections = confs  # Use confidence as selection score
                cal_logits = logits

            all_preds.append(preds.cpu())
            all_labels.append(labels.view(-1))
            all_confs.append(confs.cpu())
            all_selections.append(selections.cpu())
            all_logits.append(cal_logits.cpu())

    preds = torch.cat(all_preds)
    labels = torch.cat(all_labels)
    confs = torch.cat(all_confs)
    selections = torch.cat(all_selections)
    logits = torch.cat(all_logits)

    # === BASELINE METRICS (NO REJECTION) ===
    baseline_acc = (preds == labels).float().mean().item()
    ece = ECELoss()(logits, labels).item()

    print("\n" + "="*70)
    print("HYPOTHESIS TEST RESULTS")
    print("="*70)
    print(f"\n{'BASELINE (No Rejection)':}")
    print(f"  Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
    print(f"  ECE (Calibration): {ece:.4f}")

    # === SELECTIVE METRICS (WITH REJECTION) ===
    print(f"\n{'SELECTIVE PREDICTION (With Rejection)':}")
    print(f"{'Coverage':<12} {'Rejected %':<12} {'Selective Acc':<15} {'Acc Boost':<12}")
    print("-" * 55)

    results = []
    thresholds = np.linspace(0, 0.99, 50)

    for t in thresholds:
        accepted = selections >= t
        coverage = accepted.float().mean().item()

        if accepted.sum() > 0:
            sel_acc = (preds[accepted] == labels[accepted]).float().mean().item()
        else:
            sel_acc = 0

        results.append({
            'threshold': t,
            'coverage': coverage,
            'rejected_pct': (1 - coverage) * 100,
            'selective_acc': sel_acc,
            'acc_boost': sel_acc - baseline_acc
        })

    # Print key coverage points
    for target_cov in [0.95, 0.90, 0.85, 0.80, 0.70]:
        # Find closest coverage
        closest = min(results, key=lambda x: abs(x['coverage'] - target_cov))
        if closest['coverage'] > 0:
            print(f"{closest['coverage']*100:>10.1f}%  "
                  f"{closest['rejected_pct']:>10.1f}%  "
                  f"{closest['selective_acc']*100:>13.2f}%  "
                  f"{closest['acc_boost']*100:>+10.2f}%")

    # === HYPOTHESIS VALIDATION ===
    print("\n" + "="*70)
    print("HYPOTHESIS VALIDATION")
    print("="*70)

    # Find best accuracy boost at <=20% rejection
    valid_results = [r for r in results if r['rejected_pct'] <= 20]
    if valid_results:
        best = max(valid_results, key=lambda x: x['acc_boost'])

        print(f"\nAt {best['rejected_pct']:.1f}% rejection rate:")
        print(f"  Baseline Accuracy:  {baseline_acc*100:.2f}%")
        print(f"  Selective Accuracy: {best['selective_acc']*100:.2f}%")
        print(f"  Accuracy Boost:     {best['acc_boost']*100:+.2f}%")

        if best['acc_boost'] > 0.02:  # >2% improvement
            print(f"\nâœ… HYPOTHESIS SUPPORTED: Significant accuracy boost "
                  f"({best['acc_boost']*100:.1f}%) with low rejection ({best['rejected_pct']:.1f}%)")
        else:
            print(f"\nâš ï¸  Marginal improvement. Consider adjusting coverage target or training longer.")

    return {
        'baseline_accuracy': baseline_acc,
        'ece': ece,
        'results': results,
        'predictions': preds,
        'labels': labels,
        'selections': selections
    }


def plot_risk_coverage(results, save_path='risk_coverage.png'):
    """Plot the risk-coverage trade-off curve."""

    coverages = [r['coverage'] for r in results]
    accuracies = [r['selective_acc'] for r in results]

    plt.figure(figsize=(10, 6))
    plt.plot(coverages, accuracies, 'b-', linewidth=2, label='Selective Accuracy')
    plt.axhline(y=results[0]['selective_acc'], color='r', linestyle='--',
                label=f'Baseline (no rejection): {results[0]["selective_acc"]*100:.1f}%')

    plt.xlabel('Coverage (Fraction Accepted)', fontsize=12)
    plt.ylabel('Accuracy', fontsize=12)
    plt.title('Risk-Coverage Trade-off\n(Higher accuracy at lower coverage = better selective prediction)', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xlim(0, 1)
    plt.ylim(0, 1)

    # Add annotation for key point
    for target in [0.8, 0.9]:
        closest = min(results, key=lambda x: abs(x['coverage'] - target))
        plt.annotate(f'{closest["selective_acc"]*100:.1f}%\n@{closest["coverage"]*100:.0f}% cov',
                    xy=(closest['coverage'], closest['selective_acc']),
                    xytext=(closest['coverage']-0.1, closest['selective_acc']+0.05),
                    fontsize=9, ha='center',
                    arrowprops=dict(arrowstyle='->', color='gray'))

    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.show()
    print(f"Saved plot to {save_path}")

# ============================================================================
# MEDICAL IMAGING USE CASES
# ============================================================================

def demonstrate_medical_use_cases(model, test_loader, device):
    """
    Demonstrates practical medical imaging use cases.
    """

    print("\n" + "="*70)
    print("MEDICAL IMAGING USE CASES")
    print("="*70)

    # Get predictions
    model.eval()
    images, labels = next(iter(test_loader))
    images = images.to(device)
    labels = labels.to(device)  # Move labels to same device

    with torch.no_grad():
        cal_logits, selection, _, _ = model(images)
        probs = F.softmax(cal_logits, dim=1)
        confs, preds = torch.max(probs, dim=1)
        selection = selection.squeeze()

    # === USE CASE 1: Automated Triage ===
    print("\nðŸ“‹ USE CASE 1: AUTOMATED TRIAGE SYSTEM")
    print("-" * 50)
    print("Scenario: OCT scan analysis for retinal disease screening")
    print("Goal: Automatically classify clear cases, flag uncertain ones for specialist review")

    threshold = 0.7
    accepted = selection >= threshold

    auto_diagnose = accepted.sum().item()
    specialist_review = (~accepted).sum().item()

    print(f"\nWith threshold = {threshold}:")
    print(f"  ðŸ¤– Auto-diagnosed: {auto_diagnose} patients ({auto_diagnose/len(images)*100:.1f}%)")
    print(f"  ðŸ‘¨â€âš•ï¸ Flagged for specialist: {specialist_review} patients ({specialist_review/len(images)*100:.1f}%)")

    # Accuracy on auto-diagnosed
    if accepted.sum() > 0:
        auto_acc = (preds[accepted] == labels[accepted].view(-1)).float().mean().item()
        print(f"  âœ… Auto-diagnosis accuracy: {auto_acc*100:.2f}%")

    # === USE CASE 2: High-Confidence Diagnosis ===
    print("\nðŸŽ¯ USE CASE 2: HIGH-CONFIDENCE DIAGNOSIS")
    print("-" * 50)
    print("Scenario: Only provide diagnosis when model is highly confident")
    print("Goal: Maximize precision for critical decisions")

    threshold = 0.9
    accepted = selection >= threshold

    if accepted.sum() > 0:
        high_conf_acc = (preds[accepted] == labels[accepted].view(-1)).float().mean().item()
        coverage = accepted.float().mean().item()
        print(f"\nWith threshold = {threshold}:")
        print(f"  Coverage: {coverage*100:.1f}% of cases")
        print(f"  Accuracy on accepted: {high_conf_acc*100:.2f}%")

    # === USE CASE 3: Tiered Confidence System ===
    print("\nðŸ“Š USE CASE 3: TIERED CONFIDENCE SYSTEM")
    print("-" * 50)
    print("Scenario: Route cases based on model confidence")

    high_conf = selection >= 0.8
    medium_conf = (selection >= 0.5) & (selection < 0.8)
    low_conf = selection < 0.5

    print(f"\n  ðŸŸ¢ High confidence (â‰¥0.8):   {high_conf.sum().item()} cases - Auto-process")
    print(f"  ðŸŸ¡ Medium confidence (0.5-0.8): {medium_conf.sum().item()} cases - Quick review")
    print(f"  ðŸ”´ Low confidence (<0.5):    {low_conf.sum().item()} cases - Full specialist review")

    # === USE CASE 4: Quality Control ===
    print("\nðŸ” USE CASE 4: QUALITY CONTROL / ANOMALY DETECTION")
    print("-" * 50)
    print("Scenario: Identify potentially problematic images or unusual cases")

    # Lowest selection scores might indicate OOD or poor quality
    sorted_indices = torch.argsort(selection)
    lowest_5 = sorted_indices[:5]

    print(f"\nMost uncertain cases (potential quality issues):")
    for i, idx in enumerate(lowest_5):
        print(f"  Sample {idx.item()}: Selection score = {selection[idx].item():.3f}, "
              f"Confidence = {confs[idx].item():.3f}")

MODEL_PATH = "./models/resnet18_none.pth"
print(f"Model path exists: {os.path.exists(MODEL_PATH)}")

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":

    # Setup
    data_transform = transforms.Compose([
        transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor(),
    ])

    BATCH_SIZE = 128

    print("Loading OCTMNIST dataset...")
    train_dataset = OCTMNIST(split='train', download=True, transform=data_transform)
    val_dataset = OCTMNIST(split='val', download=True, transform=data_transform)
    test_dataset = OCTMNIST(split='test', download=True, transform=data_transform)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # ========================================================================
    # Load the pretrained model (TorchScript archive)
    # ========================================================================
    # Load TorchScript model and extract its state_dict
    scripted_model = torch.jit.load(MODEL_PATH, map_location=device)
    scripted_state = scripted_model.state_dict()
    
    # Check the fc layer structure
    print("TorchScript model fc keys:", [k for k in scripted_state.keys() if 'fc' in k])
    
    # Create a fresh resnet18 with matching architecture
    base_model = resnet18(weights=None)
    
    # Automatically detect fc structure and adapt
    if 'fc.weight' in scripted_state:
        # Simple Linear layer structure
        print("Detected: Simple Linear fc layer")
        fc_weight = scripted_state['fc.weight']
        fc_bias = scripted_state['fc.bias']
        out_features = fc_bias.shape[0]  # Number of output classes
        in_features = fc_weight.shape[1]  # Input features (should be 512)
        
        # Replace fc with a simple Linear layer to match TorchScript structure
        base_model.fc = nn.Linear(in_features, out_features)
        
    elif 'fc.0.weight' in scripted_state:
        # Sequential layer structure: fc.0 (Linear) -> fc.1 (ReLU) -> fc.2 (Linear)
        print("Detected: Sequential fc layer (fc.0 -> ReLU -> fc.2)")
        fc_0_weight = scripted_state['fc.0.weight']
        fc_2_weight = scripted_state['fc.2.weight']
        
        hidden_dim = fc_0_weight.shape[0]  # Output of first layer = hidden dimension
        in_features = fc_0_weight.shape[1]  # Input features (should be 512)
        out_features = fc_2_weight.shape[0]  # Final output classes
        
        # Reconstruct the Sequential fc layer to match TorchScript structure
        base_model.fc = nn.Sequential(
            nn.Linear(in_features, hidden_dim),  # fc.0
            nn.ReLU(),                            # fc.1
            nn.Linear(hidden_dim, out_features)   # fc.2
        )
    else:
        raise ValueError(f"Unknown fc layer structure. Keys: {[k for k in scripted_state.keys() if 'fc' in k]}")
    
    # Transfer weights from TorchScript model
    base_model.load_state_dict(scripted_state)
    base_model = base_model.to(device)
    base_model.eval()

    print(f"Loaded pretrained model from: {MODEL_PATH}")

    # Create Calibrated SelectiveNet
    model = CalibratedSelectiveNet(base_model, num_classes=4).to(device)

    # ========================================================================
    # TRAIN WITH IMPROVED HYPERPARAMETERS
    # ========================================================================
    print("\n" + "="*70)
    print("TRAINING CALIBRATED SELECTIVENET (IMPROVED HYPERPARAMETERS)")
    print("="*70)

    model, history = train_calibrated_selectivenet(
        model, train_loader, val_loader, device,
        epochs=75,           # Increased from 30
        lr=5e-4,             # Slightly lower initial LR for stability
        coverage=0.85,       # Target 85% acceptance
        weight_decay=1e-4    # Added regularization
    )

    # ========================================================================
    # EVALUATE - TEST YOUR HYPOTHESIS
    # ========================================================================
    results = evaluate_hypothesis(model, test_loader, device, method='selectivenet')

    # Plot risk-coverage curve
    plot_risk_coverage(results['results'])

    # ========================================================================
    # DEMONSTRATE USE CASES
    # ========================================================================
    demonstrate_medical_use_cases(model, test_loader, device)

    # ========================================================================
    # SAVE
    # ========================================================================
    torch.save(model.state_dict(), 'calibrated_selectivenet.pth')
    print("\n Model saved to calibrated_selectivenet.pth")
    print("Saving to:", os.path.abspath("calibrated_selectivenet.pth"))
